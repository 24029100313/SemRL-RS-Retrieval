# HarMA 架构可视化图解

## 1. 整体架构流程图

```
┌─────────────────────────────────────────────────────────────────┐
│                         输入数据                                  │
│  Image: [B, 3, 224, 224]     Text: ["caption1", "caption2", ...]│
└────────────┬────────────────────────────────┬────────────────────┘
             │                                 │
             ↓                                 ↓
┌────────────────────────┐        ┌────────────────────────┐
│   Vision Encoder       │        │   Text Tokenizer       │
│   (ViT-B/32)          │        │   (CLIP Tokenizer)     │
│   Frozen ❄️           │        └──────────┬─────────────┘
└──────────┬─────────────┘                   │
           │                                 ↓
           │                    ┌────────────────────────┐
           │                    │   Text Encoder         │
           │                    │   (CLIP Text Trans.)   │
           │                    │   Frozen ❄️            │
           │                    └──────────┬─────────────┘
           │                               │
           │ ← 注入Adapter ↓               │ ← 注入Adapter ↓
           │                               │
    ┌──────▼──────────────┐      ┌────────▼──────────────┐
    │ Layer 0             │      │ Layer 0               │
    │ ┌─────────────────┐ │      │ ┌───────────────────┐ │
    │ │ Self-Attention  │ │      │ │ Self-Attention    │ │
    │ └─────────────────┘ │      │ └───────────────────┘ │
    │ ┌─────────────────┐ │      │ ┌───────────────────┐ │
    │ │ MLP + MMadapter │◄├──────┼─┤ MLP + MMadapter   │ │
    │ │   (独立)        │ │      │ │   + BiShareAdapt  │ │
    │ └─────────────────┘ │      │ └───────────────────┘ │
    └──────┬──────────────┘      └────────┬──────────────┘
           │                               │
    ┌──────▼──────────────┐      ┌────────▼──────────────┐
    │ Layer 1             │      │ Layer 1               │
    │ (同上结构)          │◄─────┤ (同上结构)            │
    └──────┬──────────────┘      └────────┬──────────────┘
           │                               │
           ⋮                               ⋮
           │ (共12层)                      │ (共12层)
           ⋮                               ⋮
           │                               │
    ┌──────▼──────────────┐      ┌────────▼──────────────┐
    │ Layer 11            │      │ Layer 11              │
    └──────┬──────────────┘      └────────┬──────────────┘
           │                               │
           ↓                               ↓
    ┌──────────────────────┐      ┌────────────────────┐
    │ CLS Token            │      │ EOS Token          │
    │ [B, 768]             │      │ [B, 512]           │
    └──────┬───────────────┘      └────────┬───────────┘
           │                               │
           ↓                               ↓
    ┌──────────────────────┐      ┌────────────────────┐
    │ Projection           │      │ Projection         │
    │ 768 → 512            │      │ 512 → 512          │
    └──────┬───────────────┘      └────────┬───────────┘
           │                               │
           ↓                               ↓
    ┌──────────────────────┐      ┌────────────────────┐
    │ L2 Normalize         │      │ L2 Normalize       │
    └──────┬───────────────┘      └────────┬───────────┘
           │                               │
           └──────────────┬────────────────┘
                          ↓
              ┌────────────────────────┐
              │  Image Emb   Text Emb  │
              │  [B, 512]    [B, 512]  │
              └────────────┬───────────┘
                           ↓
              ┌────────────────────────┐
              │   Similarity Matrix    │
              │   S = I @ T^T          │
              │   [B, B]               │
              └────────────┬───────────┘
                           ↓
              ┌────────────────────────┐
              │   Loss Functions       │
              │  • Contrastive Loss    │
              │  • Triplet Loss        │
              └────────────────────────┘
```

---

## 2. MMadapter 详细结构

### 2.1 图像模态的 MMadapter（独立）

```
输入 x_img: [seq_len, B, 768]
     │
     ├─────────────────┐ (residual connection)
     │                 │
     ↓                 │
┌─────────────────┐    │
│ Linear Down     │    │
│ 768 → 128       │    │
└────────┬────────┘    │
         │             │
         ↓             │
┌─────────────────┐    │
│ GELU            │    │
└────────┬────────┘    │
         │             │
         ├────────┐    │ (skip connection)
         │        │    │
         ↓        │    │
┌─────────────────┐    │
│ Self-Attention  │    │
│ (8 heads, 128d) │    │
└────────┬────────┘    │
         │        │    │
         ↓        │    │
┌─────────────────┐    │
│ Self-Attention  │    │
│ (再来一次)      │    │
└────────┬────────┘    │
         │        │    │
         ↓        ↓    │
    ┌────────────────┐ │
    │ Gated Fusion   │ │
    │ α*x + (1-α)*mid│ │
    └────────┬───────┘ │
             │         │
             ↓         │
    ┌────────────────┐ │
    │ Linear Up      │ │
    │ 128 → 768      │ │
    └────────┬───────┘ │
             │         │
             ↓         ↓
            Add ───────┘
             │
             ↓
输出: [seq_len, B, 768]
```

### 2.2 文本模态的 MMadapter（共享BiShareAdapter）

```
输入 x_txt: [seq_len, B, 512]
     │
     ├─────────────────┐ (residual)
     │                 │
     ↓                 │
┌─────────────────┐    │
│ Linear Down     │    │
│ 512 → 128       │    │
└────────┬────────┘    │
         │             │
         ↓             │
┌─────────────────┐    │
│ GELU            │    │
└────────┬────────┘    │
         │             │
         ├────────┐    │ (skip connection)
         │        │    │
         ↓        │    │
┌─────────────────┐    │
│ Self-Attention  │    │
│ (8 heads, 128d) │    │
└────────┬────────┘    │
         │        │    │
         ↓        │    │
┌─────────────────┐    │
│ BiShareAdapter  │◄───┼─── 与图像模态共享
│  (跨模态桥梁)   │    │
└────────┬────────┘    │
         │        │    │
         ↓        │    │
┌─────────────────┐    │
│ Self-Attention  │    │
└────────┬────────┘    │
         │        │    │
         ↓        ↓    │
    ┌────────────────┐ │
    │ Gated Fusion   │ │
    └────────┬───────┘ │
             │         │
             ↓         │
    ┌────────────────┐ │
    │ Linear Up      │ │
    │ 128 → 512      │ │
    └────────┬───────┘ │
             │         │
             ↓         ↓
            Add ───────┘
             │
             ↓
输出: [seq_len, B, 512]
```

---

## 3. BiShareAdapter 内部结构

```
输入 x: [seq_len, B, 128]
     │
     ├───────────────┐ (residual)
     │               │
     ↓               │
┌─────────────────┐  │
│ Linear Down     │  │
│ 128 → 64        │  │
└────────┬────────┘  │
         │           │
         ├──────┐    │ (skip for gated fusion)
         │      │    │
         ↓      │    │
┌─────────────────┐  │
│ Multi-head Attn │  │
│ (8 heads, 64d)  │  │
└────────┬────────┘  │
         │      │    │
         ↓      │    │
┌─────────────────┐  │
│ GELU            │  │
└────────┬────────┘  │
         │      │    │
         ↓      ↓    │
    ┌────────────────┐
    │ Gated Fusion   │
    │ gate = σ(0.6)  │  ← 可学习参数
    │ α*attn+(1-α)*x │
    └────────┬───────┘
             │        │
             ↓        │
    ┌────────────────┐│
    │ Linear Up      ││
    │ 64 → 128       ││
    └────────┬───────┘│
             │        │
             ↓        ↓
            Add ──────┘
             │
             ↓
输出: [seq_len, B, 128]
```

**关键点**：
- BiShareAdapter在12层中是**共享**的
- 文本侧的MMadapter通过这个共享模块与图像侧隐式交互
- 参数量：64×128×2 + 64×64×8×3 ≈ 140K（每个）

---

## 4. 训练流程时序图

```
Epoch 1:
┌────────────┐
│ Start      │
└─────┬──────┘
      │
      ↓
┌─────────────────────────────────────┐
│ Batch 1: Load (images, texts)       │
└─────┬───────────────────────────────┘
      │
      ├─→ Forward Pass
      │    ├─ Image → Vision Encoder → [B,512]
      │    ├─ Text → Text Encoder → [B,512]
      │    └─ Similarity = Image @ Text^T
      │
      ├─→ Compute Loss
      │    ├─ L_contr = InfoNCE(sim_matrix)
      │    └─ L_triplet = Weighted_Triplet(sim_matrix)
      │
      ├─→ Backward
      │    └─ 只更新Adapter参数 (1-5%)
      │
      └─→ Step Optimizer & Scheduler
           │
           ↓
┌─────────────────────────────────────┐
│ Batch 2: ...                        │
└─────┬───────────────────────────────┘
      ⋮
      │
      ↓
┌─────────────────────────────────────┐
│ Batch N: End of Epoch               │
└─────┬───────────────────────────────┘
      │
      ↓
┌─────────────────────────────────────┐
│ Evaluation                          │
│  1. Extract all image features      │
│  2. Extract all text features       │
│  3. Compute similarity matrix       │
│  4. Calculate R@1, R@5, R@10        │
└─────┬───────────────────────────────┘
      │
      ↓
┌─────────────────────────────────────┐
│ Save Checkpoint if Best             │
└─────┬───────────────────────────────┘
      │
      ↓
Epoch 2: ...
```

---

## 5. 损失函数计算流程

### 5.1 对比学习损失 (Contrastive Loss)

```
输入: image_emb [B, 512], text_emb [B, 512]

Step 1: 跨GPU聚合
┌────────────────────────────────────┐
│ GPU 0: [B, 512]                    │
│ GPU 1: [B, 512]                    │
├───────────────────────────────────→│
│ All-Gather                         │
└────────────────────────────────────┘
        ↓
┌────────────────────────────────────┐
│ image_all: [B*world_size, 512]    │
│ text_all: [B*world_size, 512]     │
└────────────────────────────────────┘

Step 2: 计算相似度矩阵
logits = (image_all @ text_all.T) / τ
         [B*W, B*W]
         
         Text 0   Text 1   ...  Text B*W
Image 0   s_00     s_01    ...   s_0n
Image 1   s_10     s_11    ...   s_1n
  ...      ...      ...    ...    ...
Image n   s_n0     s_n1    ...   s_nn
          ↑
        正样本 (对角线)

Step 3: 计算损失
labels = [0, 1, 2, ..., B*W-1]

L_i2t = CrossEntropy(logits, labels)
      = -Σ log(exp(s_ii) / Σ_j exp(s_ij))
      
L_t2i = CrossEntropy(logits.T, labels)

L_contr = (L_i2t + L_t2i) / 2
```

### 5.2 加权三元组损失 (Weighted Triplet Loss)

```
输入: scores = image_all @ text_all.T  [B*W, B*W]

Step 1: 提取对角线（正样本相似度）
diagonal = diag(scores)  [B*W]

示例 (B*W=4):
scores = [[0.9, 0.3, 0.2, 0.1],   diagonal = [0.9,
          [0.2, 0.8, 0.4, 0.3],               0.8,
          [0.1, 0.2, 0.9, 0.5],               0.9,
          [0.3, 0.4, 0.3, 0.7]]               0.7]

Step 2: 广播并计算margin
d1 = diagonal.expand_as(scores)
   = [[0.9, 0.9, 0.9, 0.9],
      [0.8, 0.8, 0.8, 0.8],
      [0.9, 0.9, 0.9, 0.9],
      [0.7, 0.7, 0.7, 0.7]]

cost_s = max(0, margin + scores - d1)
       = max(0, 0.2 + scores - diagonal_row)
       
示例：
cost_s = [[0,   0,   0,   0  ],  ← 对角线为0
          [0,   0,   0.2, 0.1],
          [0,   0,   0,   0  ],
          [0.2, 0.3, 0.2, 0  ]]
          
Step 3: 计算Focal权重
p = exp(-cost_s)
weight = (1 - p)^γ  (γ=2.0)

难样本 → cost_s大 → p小 → weight大

Step 4: 加权求和
L_triplet = Σ(weight * cost_s)
```

---

## 6. 评估流程图

```
测试集: N_img 张图像, N_txt 条文本

┌────────────────────────────────────┐
│ Step 1: Extract Image Features     │
│ for batch in image_loader:         │
│     img_emb = model.encode_image() │
│     features.append(img_emb)       │
│ image_features: [N_img, 512]       │
└─────────────┬──────────────────────┘
              │
              ↓
┌────────────────────────────────────┐
│ Step 2: Extract Text Features      │
│ for batch in text_batches:         │
│     txt_emb = model.encode_text()  │
│     features.append(txt_emb)       │
│ text_features: [N_txt, 512]        │
└─────────────┬──────────────────────┘
              │
              ↓
┌────────────────────────────────────┐
│ Step 3: Compute Similarity Matrix  │
│ sims = image_feat @ text_feat.T    │
│        [N_img, N_txt]              │
└─────────────┬──────────────────────┘
              │
              ↓
┌────────────────────────────────────┐
│ Step 4: Image→Text Retrieval       │
│ for each image:                    │
│   排序: 相似度从高到低              │
│   找到第一个正样本的rank            │
│ 计算 R@1, R@5, R@10                │
└─────────────┬──────────────────────┘
              │
              ↓
┌────────────────────────────────────┐
│ Step 5: Text→Image Retrieval       │
│ (类似步骤4)                         │
└─────────────┬──────────────────────┘
              │
              ↓
┌────────────────────────────────────┐
│ Step 6: 计算平均指标                │
│ r_mean = (TR_mean + IR_mean) / 2   │
└────────────────────────────────────┘
```

**示例**：
```
假设测试集有1000张图像，5000条文本（每图5条）

相似度矩阵: [1000, 5000]
         Text 0  Text 1  Text 2  ...  Text 4999
Image 0   0.95    0.94    0.91   ...   0.12
Image 1   0.23    0.22    0.19   ...   0.88
  ...      ...     ...     ...   ...    ...
          ↑       ↑       ↑            ↑
        正样本(同一图像的描述)

Image 0的评估:
- 排序: [0, 1, 2, ...] 按相似度降序
- 正样本索引: [0, 1, 2, 3, 4]
- 找到第一个正样本的rank: 0 (最高)
- R@1 ✓, R@5 ✓, R@10 ✓
```

---

## 7. 参数共享示意图

```
12 Layers 的参数共享策略:

┌──────────────────────────────────────────────────────────────┐
│                    Layer 0                                   │
├───────────────────┬──────────────────────────────────────────┤
│ Vision Tower      │ Text Tower                               │
│                   │                                          │
│ ┌───────────────┐ │ ┌───────────────┐   ┌───────────────┐  │
│ │ Self-Attention│ │ │ Self-Attention│   │BiShareAdapter │  │
│ └───────────────┘ │ └───────────────┘   │   (独立实例)  │  │
│ ┌───────────────┐ │ ┌───────────────┐   └────────┬──────┘  │
│ │ MLP           │ │ │ MLP           │            │         │
│ └───────────────┘ │ └───────────────┘            │         │
│ ┌───────────────┐ │ ┌───────────────┐            │         │
│ │  MMadapter_0  │◄┼─┤  MMadapter_0  │←───────────┘         │
│ │  (Vision)     │ │ │  (Text)       │  引用BiShareAdapter  │
│ └───────────────┘ │ └───────────────┘                      │
└───────────────────┴──────────────────────────────────────────┘
         ⋮                         ⋮
┌──────────────────────────────────────────────────────────────┐
│                    Layer 11                                  │
├───────────────────┬──────────────────────────────────────────┤
│ ┌───────────────┐ │ ┌───────────────┐   ┌───────────────┐  │
│ │ MMadapter_11  │◄┼─┤ MMadapter_11  │←──┤BiShareAdapter │  │
│ │  (Vision)     │ │ │  (Text)       │   │   (独立实例)  │  │
│ └───────────────┘ │ └───────────────┘   └───────────────┘  │
└───────────────────┴──────────────────────────────────────────┘

总结:
• Vision侧: 12个独立的MMadapter (无BiShareAdapter)
• Text侧: 12个MMadapter + 12个BiShareAdapter
• 共享方式: Text的MMadapter[i] 引用 BiShareAdapter[i]
• 参数量: 
  - Vision: 12 × MMadapter参数 ≈ 1.5M
  - Text: 12 × (MMadapter + BiShareAdapter) ≈ 2.0M
  - 总计: 约3.5M (占CLIP总参数的1-2%)
```

---

## 8. 配置文件参数影响图

```
config.yaml 的关键参数如何影响模型:

┌─────────────────┐
│ is_harma: True  │ ──→ 决定是否使用HarMA框架
└────────┬────────┘     (False则使用原始CLIP)
         │
         ↓
┌─────────────────┐
│ model: 'geo'    │ ──→ 'geo': 加载GeoRSCLIP预训练
│   or 'vit'      │     'vit': 使用OpenAI CLIP
└────────┬────────┘
         │
         ↓
┌─────────────────┐
│ embed_dim: 512  │ ──→ 特征维度（不可改，CLIP固定）
└────────┬────────┘
         │
         ↓
┌──────────────────────┐
│ temp1: 0.07          │ ──→ 对比学习温度参数
│ (可学习)             │     (小→相似度差异大)
└────────┬─────────────┘
         │
         ↓
┌──────────────────────┐
│ use_triplet_loss:    │ ──→ 损失函数选择
│   False              │     • False: Contr + Triplet
│ use_affil_loss:      │     • Triplet: 只用三元组
│   False              │     • Affil: 类中心对齐
└────────┬─────────────┘
         │
         ↓
┌──────────────────────┐
│ batch_size_train:214 │ ──→ 训练批次大小
│ (per GPU)            │     (越大越稳定，需更多显存)
└────────┬─────────────┘
         │
         ↓
┌──────────────────────┐
│ lr: 4e-6 (geo)       │ ──→ 学习率
│     4e-4 (vit)       │     • Geo更小（已预训练）
└────────┬─────────────┘     • ViT需要更多探索
         │
         ↓
┌──────────────────────┐
│ epochs: 80 (geo)     │ ──→ 训练轮数
│         50 (vit)     │     (根据数据集大小调整)
└──────────────────────┘
```

---

## 9. 典型的前向传播数据流

```
输入: batch_size=32

Image: [32, 3, 224, 224]
Text: ["caption1", "caption2", ..., "caption32"]

───────────────────────────────────────────────────────

Vision Encoder:
  Input: [32, 3, 224, 224]
    ↓
  Patch Embedding: [32, 3, 224, 224] → [32, 49, 768]
    ↓
  + Positional Embedding: [32, 50, 768] (加[CLS])
    ↓
  Layer 0:
    Self-Attn: [32, 50, 768]
      ↓
    MLP: [32, 50, 768]
      ↓
    MMadapter: [32, 50, 768]
  ⋮
  Layer 11: (同上)
    ↓
  Output: [32, 50, 768]
    ↓
  Take [CLS]: [32, 768]
    ↓
  Projection: [32, 768] → [32, 512]
    ↓
  L2 Normalize: [32, 512]

───────────────────────────────────────────────────────

Text Encoder:
  Input: ["caption1", ...]
    ↓
  Tokenize: [32, 77] (77=max length)
    ↓
  Token Embedding: [32, 77, 512]
    ↓
  + Positional Embedding: [32, 77, 512]
    ↓
  Layer 0:
    Self-Attn: [32, 77, 512]
      ↓
    MLP: [32, 77, 512]
      ↓
    MMadapter (引用BiShareAdapter): [32, 77, 512]
  ⋮
  Layer 11: (同上)
    ↓
  Output: [32, 77, 512]
    ↓
  Take [EOS]: [32, 512]
    ↓
  Projection: [32, 512] → [32, 512]
    ↓
  L2 Normalize: [32, 512]

───────────────────────────────────────────────────────

Loss Computation:
  image_emb: [32, 512]
  text_emb: [32, 512]
    ↓
  All-Gather (2 GPUs): [64, 512] each
    ↓
  Similarity: [64, 64]
    ↓
  L_contr: scalar
  L_triplet: scalar
    ↓
  L_total = L_contr + L_triplet
```

---

## 10. 改进方向建议（为你的创新准备）

### 方向1: 改进Adapter结构
```
当前: MMadapter (Down128 → Attn → Up)
       ↓
可尝试:
  • 动态路由: 根据输入自适应选择Adapter路径
  • 多尺度融合: 不同层的Adapter输出融合
  • 条件Adapter: 基于任务/数据集的条件激活
```

### 方向2: 改进损失函数
```
当前: Contr + Weighted Triplet
       ↓
可尝试:
  • 细粒度对齐: 词-区域级别的对比学习
  • 语义增强: 利用语义标签的监督信号
  • 难负样本挖掘: 动态调整margin和weight
```

### 方向3: 数据增强
```
当前: 标准CLIP数据增强
       ↓
可尝试:
  • 混合增强: MixUp/CutMix for images
  • 文本增强: 同义词替换、回译
  • 跨模态增强: 用文本指导图像增强
```

### 方向4: 架构创新
```
当前: 单向适配 (Adapter插入Transformer)
       ↓
可尝试:
  • 双向交互: 图像-文本显式交互层
  • 层次融合: 不同层特征的加权组合
  • 注意力机制: Cross-Attention between modalities
```

---

**最后更新**: 2025-10-18
**配合阅读**: CODE_ANALYSIS.md

