"""
è¯è¡¨è¦†ç›–ç‡æ£€æŸ¥è„šæœ¬
åˆ†æè¯è¡¨åœ¨çœŸå®è®­ç»ƒæ•°æ®ä¸­çš„è¦†ç›–ç‡

åŠŸèƒ½ï¼š
1. è®¡ç®—æ¯ä¸ªç²’åº¦çš„è¯è¡¨è¦†ç›–ç‡
2. æ‰¾å‡ºé«˜é¢‘ä½†æœªè¢«è¯è¡¨è¦†ç›–çš„è¯æ±‡
3. éšæœºæŠ½æ ·captionè¿›è¡Œäººå·¥éªŒè¯

ä½œè€…ï¼šSean R. Liang
æ—¥æœŸï¼š2025-10-18
"""

import json
import random
from collections import Counter
import spacy

# åŠ è½½spacyæ¨¡å‹
print("åŠ è½½spacyæ¨¡å‹...")
try:
    nlp = spacy.load("en_core_web_sm")
    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\n")
except:
    print("âŒ é”™è¯¯ï¼šspacyæ¨¡å‹æœªå®‰è£…")
    print("è¯·è¿è¡Œï¼špython -m spacy download en_core_web_sm")
    exit(1)


# ====================================================================================
# è¯å½¢å½’ä¸€æ˜ å°„è¡¨
# ====================================================================================

# å•å¤æ•°æ˜ å°„ï¼ˆè¯è¡¨å¤æ•° â†’ è¯­æ–™å•æ•°ï¼‰
PLURAL_SINGULAR_MAP = {
    'roofs': 'roof',
    'pools': 'pool',
    'lands': 'land',
    'barelands': 'bareland',
    'viaducts': 'viaduct',
    'factories': 'factory',
    'warehouses': 'warehouse',
    'farmlands': 'farmland',
    'parks': 'park',
    'airports': 'airport',
    'bridges': 'bridge',
    'buildings': 'building',
    'houses': 'house',
    'roads': 'road',
    'cars': 'car',
    'planes': 'plane',
    'ships': 'ship',
    'trees': 'tree',
    'fields': 'field',
}

# å¤åˆçŸ­è¯­æ˜ å°„ï¼ˆè¯­æ–™ç©ºæ ¼å½¢å¼ â†’ è¯è¡¨è¿å†™å½¢å¼ï¼‰
PHRASE_CANONICAL_MAP = {
    'storage tank': 'storagetanks',
    'storage tanks': 'storagetanks',
    'baseball field': 'baseballfield',
    'basketball court': 'basketballcourt',
    'football field': 'footballfield',
    'swimming pool': 'swimmingpool',
    'tennis court': 'tennis',  # è¯è¡¨ä¸­æœ‰tennis
    'parking lot': 'parking',
}

# è‹±æ–‡åœç”¨è¯ï¼ˆç²¾ç®€ç‰ˆï¼Œå»æ‰ç©ºé—´ä»‹è¯ï¼‰
STOPWORDS = {
    'a', 'an', 'the', 'and', 'or', 'but',
    'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had',
    'do', 'does', 'did',
    'will', 'would', 'should', 'could', 'may', 'might', 'must',
    'this', 'that', 'these', 'those',
    'it', 'its', 'they', 'them', 'their',
    'there',
}

# éœ€è¦å¿½ç•¥çš„åŠ¨è¯åˆ†è¯/æ—¶æ€ï¼ˆéåœ°ç‰©/åœºæ™¯/ç©ºé—´å…³ç³»ï¼‰
VERB_FORMS_TO_IGNORE = {
    'parked', 'planted', 'located', 'built', 'arranged',
    'scattered', 'lined', 'surrounded', 'connected',
    'painted', 'covered', 'shaped', 'curved'
}


def normalize_word(word, pos):
    """
    è¯å½¢å½’ä¸€åŒ–
    
    å‚æ•°ï¼š
        word: åŸå§‹è¯
        pos: è¯æ€§æ ‡ç­¾
        
    è¿”å›ï¼š
        normalized_word: å½’ä¸€åŒ–åçš„è¯
        should_count: æ˜¯å¦åº”è¯¥è®¡å…¥å†…å®¹è¯ç»Ÿè®¡
    """
    word_lower = word.lower()
    
    # 1. åœç”¨è¯è¿‡æ»¤
    if word_lower in STOPWORDS:
        return word_lower, False
    
    # 2. åŠ¨è¯åˆ†è¯è¿‡æ»¤
    if word_lower in VERB_FORMS_TO_IGNORE:
        return word_lower, False
    
    # 3. åªä¿ç•™åè¯/å½¢å®¹è¯/å‰¯è¯/ä»‹è¯ï¼ˆå¯¹åº”Object/Scene/Layoutï¼‰
    if pos not in ['NOUN', 'PROPN', 'ADJ', 'ADV', 'ADP']:
        return word_lower, False
    
    # 4. å•å¤æ•°æ˜ å°„
    if word_lower in PLURAL_SINGULAR_MAP:
        return PLURAL_SINGULAR_MAP[word_lower], True
    
    return word_lower, True


def load_vocabulary(vocab_path):
    """åŠ è½½è¯è¡¨"""
    with open(vocab_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return {
        'object': set(data['object']),
        'scene': set(data['scene']),
        'layout': set(data['layout'])
    }


def load_captions(json_paths):
    """åŠ è½½æ‰€æœ‰caption"""
    all_captions = []
    for json_path in json_paths:
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            captions = [item['caption'].lower().strip() for item in data]
            all_captions.extend(captions)
            print(f"âœ“ åŠ è½½ {len(captions)} æ¡caption from {json_path}")
        except Exception as e:
            print(f"âš  è·³è¿‡ {json_path}: {e}")
    return all_captions


def analyze_coverage(captions, vocabulary, nlp):
    """
    åˆ†æè¯è¡¨è¦†ç›–ç‡ï¼ˆåŒé‡å£å¾„ï¼šall + contentï¼‰
    
    è¿”å›ï¼š
        coverage_stats: è¦†ç›–ç‡ç»Ÿè®¡
        uncovered_words: æœªè¦†ç›–çš„é«˜é¢‘è¯
        covered_words: å·²è¦†ç›–çš„è¯ç»Ÿè®¡
    """
    print("\nåˆ†æè¯è¡¨è¦†ç›–ç‡ï¼ˆå†…å®¹è¯å£å¾„ + è¯å½¢å½’ä¸€ï¼‰...")
    
    # æ‰©å±•è¯è¡¨ï¼ˆåŠ å…¥å•å¤æ•°å’ŒçŸ­è¯­æ˜ å°„çš„åå‘æŸ¥æ‰¾ï¼‰
    extended_vocabulary = {}
    for granularity, vocab_set in vocabulary.items():
        extended_set = set(vocab_set)
        # åŠ å…¥å•å¤æ•°æ˜ å°„çš„ä¸¤ç«¯
        for plural, singular in PLURAL_SINGULAR_MAP.items():
            if plural in vocab_set:
                extended_set.add(singular)
            if singular in vocab_set:
                extended_set.add(plural)
        extended_vocabulary[granularity] = extended_set
    
    # ç»Ÿè®¡å˜é‡
    all_words = Counter()                     # å…¨éƒ¨è¯
    content_words = Counter()                  # å†…å®¹è¯
    covered_words_all = {g: Counter() for g in ['object', 'scene', 'layout']}
    covered_words_content = {g: Counter() for g in ['object', 'scene', 'layout']}
    
    total_tokens_all = 0
    total_tokens_content = 0
    
    for i, caption in enumerate(captions):
        if (i + 1) % 5000 == 0:
            print(f"  å¤„ç†è¿›åº¦: {i+1}/{len(captions)}")
        
        # 1. çŸ­è¯­çº§åŒ¹é…ï¼ˆä¼˜å…ˆï¼‰
        caption_lower = caption.lower()
        matched_phrases = set()
        for phrase, canonical in PHRASE_CANONICAL_MAP.items():
            if phrase in caption_lower:
                matched_phrases.add(phrase)
                # æ£€æŸ¥canonicalåœ¨å“ªä¸ªç²’åº¦
                for granularity, vocab_set in extended_vocabulary.items():
                    if canonical in vocab_set:
                        covered_words_all[granularity][canonical] += 1
                        covered_words_content[granularity][canonical] += 1
        
        # 2. åˆ†è¯çº§åŒ¹é…
        doc = nlp(caption)
        
        for token in doc:
            if token.is_punct or token.is_space:
                continue
            
            # è·³è¿‡å·²ç»è¢«çŸ­è¯­åŒ¹é…çš„è¯
            if any(token.text.lower() in phrase for phrase in matched_phrases):
                continue
            
            word_raw = token.text.lower()
            pos = token.pos_
            
            # åŸå§‹è¯ç»Ÿè®¡ï¼ˆallå£å¾„ï¼‰
            all_words[word_raw] += 1
            total_tokens_all += 1
            
            # è¯å½¢å½’ä¸€åŒ–
            word_normalized, should_count = normalize_word(word_raw, pos)
            
            # å†…å®¹è¯ç»Ÿè®¡ï¼ˆcontentå£å¾„ï¼‰
            if should_count:
                content_words[word_normalized] += 1
                total_tokens_content += 1
            
            # æ£€æŸ¥æ˜¯å¦åœ¨è¯è¡¨ä¸­ï¼ˆallå£å¾„ï¼‰
            for granularity, vocab_set in extended_vocabulary.items():
                if word_raw in vocab_set or word_normalized in vocab_set:
                    covered_words_all[granularity][word_normalized] += 1
                    if should_count:
                        covered_words_content[granularity][word_normalized] += 1
                    break
    
    # è®¡ç®—è¦†ç›–ç‡ï¼ˆåŒé‡å£å¾„ï¼‰
    total_covered_all = sum(sum(c.values()) for c in covered_words_all.values())
    total_covered_content = sum(sum(c.values()) for c in covered_words_content.values())
    
    coverage_rate_all = (total_covered_all / total_tokens_all * 100) if total_tokens_all > 0 else 0
    coverage_rate_content = (total_covered_content / total_tokens_content * 100) if total_tokens_content > 0 else 0
    
    # æ‰¾å‡ºæœªè¦†ç›–çš„é«˜é¢‘è¯ï¼ˆåŸºäºcontentå£å¾„ï¼Œå‡ºç°>20æ¬¡ï¼‰
    all_vocab = set()
    for v in extended_vocabulary.values():
        all_vocab.update(v)
    
    uncovered_content = {w: c for w, c in content_words.items() 
                         if w not in all_vocab and c >= 20}
    uncovered_sorted = sorted(uncovered_content.items(), key=lambda x: x[1], reverse=True)
    
    coverage_stats = {
        'total_tokens_all': total_tokens_all,
        'total_tokens_content': total_tokens_content,
        'covered_tokens_all': total_covered_all,
        'covered_tokens_content': total_covered_content,
        'coverage_rate_all': coverage_rate_all,
        'coverage_rate_content': coverage_rate_content,
        'unique_words_all': len(all_words),
        'unique_words_content': len(content_words),
        'covered_unique_all': len([w for w in all_words if w in all_vocab]),
        'covered_unique_content': len([w for w in content_words if w in all_vocab]),
        'granularity_stats': {
            g: {
                'vocab_size': len(vocabulary[g]),
                'used_vocab_all': len(covered_words_all[g]),
                'used_vocab_content': len(covered_words_content[g]),
                'token_count_all': sum(covered_words_all[g].values()),
                'token_count_content': sum(covered_words_content[g].values())
            }
            for g in ['object', 'scene', 'layout']
        }
    }
    
    return coverage_stats, uncovered_sorted[:50], covered_words_content


def sample_captions_analysis(captions, vocabulary, nlp, n=5):
    """éšæœºæŠ½æ ·næ¡captionè¿›è¡Œè¯¦ç»†åˆ†æ"""
    print(f"\néšæœºæŠ½æ · {n} æ¡captionè¿›è¡Œäººå·¥éªŒè¯...")
    print("=" * 70)
    
    samples = random.sample(captions, min(n, len(captions)))
    
    for i, caption in enumerate(samples, 1):
        print(f"\n[æ ·æœ¬ {i}] {caption}")
        print("-" * 70)
        
        doc = nlp(caption)
        words = [token.text.lower() for token in doc if not token.is_punct and not token.is_space]
        
        matched = {'object': [], 'scene': [], 'layout': []}
        unmatched = []
        
        for word in words:
            found = False
            for granularity, vocab_set in vocabulary.items():
                if word in vocab_set:
                    matched[granularity].append(word)
                    found = True
                    break
            if not found:
                unmatched.append(word)
        
        total_words = len(words)
        matched_count = sum(len(m) for m in matched.values())
        coverage = (matched_count / total_words * 100) if total_words > 0 else 0
        
        print(f"  Object: {', '.join(matched['object']) if matched['object'] else '(æ— )'}")
        print(f"  Scene: {', '.join(matched['scene']) if matched['scene'] else '(æ— )'}")
        print(f"  Layout: {', '.join(matched['layout']) if matched['layout'] else '(æ— )'}")
        print(f"  æœªåŒ¹é…: {', '.join(unmatched) if unmatched else '(æ— )'}")
        print(f"  è¦†ç›–ç‡: {matched_count}/{total_words} = {coverage:.1f}%")
    
    print("\n" + "=" * 70)


def print_report(coverage_stats, uncovered_words):
    """æ‰“å°è¯¦ç»†æŠ¥å‘Šï¼ˆåŒé‡å£å¾„ï¼‰"""
    print("\n" + "=" * 70)
    print("è¯è¡¨è¦†ç›–ç‡åˆ†ææŠ¥å‘Š")
    print("=" * 70)
    
    print(f"\nã€æ•´ä½“è¦†ç›–ç‡ - åŒé‡å£å¾„å¯¹æ¯”ã€‘")
    print(f"\n  1ï¸âƒ£ All Token å£å¾„ï¼ˆåŒ…å«åœç”¨è¯/åŠ©è¯ï¼‰ï¼š")
    print(f"     æ€»tokenæ•°: {coverage_stats['total_tokens_all']:,}")
    print(f"     è¢«è¯è¡¨è¦†ç›–: {coverage_stats['covered_tokens_all']:,}")
    print(f"     è¦†ç›–ç‡: {coverage_stats['coverage_rate_all']:.2f}%")
    
    print(f"\n  2ï¸âƒ£ Content Word å£å¾„ï¼ˆä»…å†…å®¹è¯ï¼šåè¯/å½¢å®¹è¯/å‰¯è¯/ä»‹è¯ï¼‰ï¼š")
    print(f"     æ€»å†…å®¹è¯tokenæ•°: {coverage_stats['total_tokens_content']:,}")
    print(f"     è¢«è¯è¡¨è¦†ç›–: {coverage_stats['covered_tokens_content']:,}")
    print(f"     â˜…â˜…â˜… å†…å®¹è¯è¦†ç›–ç‡: {coverage_stats['coverage_rate_content']:.2f}% â˜…â˜…â˜…")
    
    print(f"\nã€å”¯ä¸€è¯æ±‡ç»Ÿè®¡ã€‘")
    print(f"  Allå£å¾„ï¼š{coverage_stats['unique_words_all']:,} ä¸ªè¯ â†’ è¦†ç›– {coverage_stats['covered_unique_all']:,} ä¸ª "
          f"({coverage_stats['covered_unique_all']/coverage_stats['unique_words_all']*100:.1f}%)")
    print(f"  Contentå£å¾„ï¼š{coverage_stats['unique_words_content']:,} ä¸ªè¯ â†’ è¦†ç›– {coverage_stats['covered_unique_content']:,} ä¸ª "
          f"({coverage_stats['covered_unique_content']/coverage_stats['unique_words_content']*100:.1f}%)")
    
    print(f"\nã€å„ç²’åº¦ç»Ÿè®¡ã€‘")
    print(f"  {'ç²’åº¦':<10} {'è¯è¡¨å¤§å°':<10} {'ä½¿ç”¨(All)':<12} {'ä½¿ç”¨(Content)':<15} {'token(Content)':<15}")
    print("  " + "-" * 70)
    for granularity in ['object', 'scene', 'layout']:
        stats = coverage_stats['granularity_stats'][granularity]
        print(f"  {granularity.capitalize():<10} {stats['vocab_size']:<10} "
              f"{stats['used_vocab_all']:<12} {stats['used_vocab_content']:<15} {stats['token_count_content']:<15}")
    
    print(f"\nã€æœªè¦†ç›–çš„é«˜é¢‘å†…å®¹è¯ï¼ˆTop 30ï¼‰ã€‘")
    print("  è¿™äº›è¯æ˜¯å†…å®¹è¯ï¼ˆå/å½¢/å‰¯/ä»‹ï¼‰ï¼Œå‡ºç°é¢‘ç‡>=20æ¬¡ï¼Œä½†æœªè¢«è¯è¡¨æ”¶å½•")
    print("  " + "-" * 60)
    for i, (word, count) in enumerate(uncovered_words[:30], 1):
        print(f"  {i:2}. {word:<20} (å‡ºç° {count} æ¬¡)")
    
    # åˆ¤æ–­ç­‰çº§ï¼ˆåŸºäºContentå£å¾„ï¼‰
    rate = coverage_stats['coverage_rate_content']
    if rate >= 70:
        grade = "ä¼˜ç§€ âœ“âœ“âœ“"
        color = "ğŸŸ©"
    elif rate >= 60:
        grade = "è‰¯å¥½ âœ“âœ“"
        color = "ğŸŸ¨"
    elif rate >= 50:
        grade = "åŠæ ¼ âœ“"
        color = "ğŸŸ§"
    else:
        grade = "éœ€æ”¹è¿› âœ—"
        color = "ğŸŸ¥"
    
    print(f"\nã€æœ€ç»ˆè¯„çº§ï¼ˆContentå£å¾„ï¼‰ã€‘")
    print(f"  {color} å†…å®¹è¯è¦†ç›–ç‡ {rate:.2f}% â†’ {grade}")
    print()


def main():
    print("=" * 70)
    print("è¯è¡¨è¦†ç›–ç‡æ£€æŸ¥å·¥å…· v2.0")
    print("ï¼ˆå†…å®¹è¯å£å¾„ + è¯å½¢å½’ä¸€ + çŸ­è¯­åŒ¹é…ï¼‰")
    print("=" * 70)
    print()
    
    # é…ç½®
    vocab_path = "data/vocabulary/rs_vocabulary_v1.3.json"
    caption_paths = [
        "data/finetune/rsitmd_train.json",
        "data/finetune/rsicd_train.json"
    ]
    
    # åŠ è½½è¯è¡¨
    print(f"[1] åŠ è½½è¯è¡¨: {vocab_path}")
    vocabulary = load_vocabulary(vocab_path)
    print(f"  âœ“ Object: {len(vocabulary['object'])} ä¸ª")
    print(f"  âœ“ Scene: {len(vocabulary['scene'])} ä¸ª")
    print(f"  âœ“ Layout: {len(vocabulary['layout'])} ä¸ª")
    print()
    
    # åŠ è½½caption
    print(f"[2] åŠ è½½è®­ç»ƒé›†caption...")
    captions = load_captions(caption_paths)
    print(f"  âœ“ æ€»è®¡: {len(captions)} æ¡caption")
    
    # åˆ†æè¦†ç›–ç‡
    print(f"\n[3] åˆ†æè¦†ç›–ç‡ï¼ˆåŒé‡å£å¾„ï¼‰...")
    coverage_stats, uncovered_words, covered_words = analyze_coverage(captions, vocabulary, nlp)
    
    # æ‰“å°æŠ¥å‘Š
    print_report(coverage_stats, uncovered_words)
    
    # éšæœºæŠ½æ ·
    print(f"\n[4] éšæœºæŠ½æ ·éªŒè¯...")
    sample_captions_analysis(captions, vocabulary, nlp, n=5)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "data/vocabulary/coverage_report_v1.3.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        # æ·»åŠ uncovered_wordsåˆ°æŠ¥å‘Šä¸­
        coverage_stats['uncovered_top50'] = [
            {'word': w, 'count': c} for w, c in uncovered_words
        ]
        coverage_stats['methodology'] = {
            'version': '2.0',
            'features': [
                'Dual metrics: all tokens vs. content words only',
                'Lemmatization with singular-plural mapping',
                'Phrase-level matching (e.g., football field â†’ footballfield)',
                'POS filtering (NOUN/ADJ/ADV/ADP only for content words)',
                'Stopword removal'
            ]
        }
        json.dump(coverage_stats, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    print()


if __name__ == "__main__":
    random.seed(42)  # å›ºå®šéšæœºç§å­ï¼Œä¾¿äºå¤ç°
    main()

